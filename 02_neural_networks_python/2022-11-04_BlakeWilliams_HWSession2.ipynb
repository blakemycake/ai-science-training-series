{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "662a93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19878bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da412dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n",
      "\n",
      "MNIST data loaded: train: 60000 test: 10000\n",
      "X_train: (60000, 784)\n",
      "y_train: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# repeating the data prep from the previous notebook\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(numpy.float32)\n",
    "x_test  = x_test.astype(numpy.float32)\n",
    "\n",
    "x_train /= 255.\n",
    "x_test  /= 255.\n",
    "\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], numpy.prod(x_train[0,:,:].shape))\n",
    "x_test = x_test.reshape(x_test.shape[0], numpy.prod(x_test[0,:,:].shape))\n",
    "\n",
    "print(x_train.shape)\n",
    "y_train = y_train.astype(numpy.int32)\n",
    "y_test  = y_test.astype(numpy.int32)\n",
    "\n",
    "print()\n",
    "print('MNIST data loaded: train:',len(x_train),'test:',len(x_test))\n",
    "print('X_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "\n",
    "# one-hot encoding:\n",
    "nb_classes = 10\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302994b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import an implementation of a two-layer neural network \n",
    "# this code is based on pieces of the first assignment from Stanford's CSE231n course, \n",
    "# hosted at https://github.com/cs231n/cs231n.github.io with the MIT license\n",
    "from fc_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e00e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = x_train.shape[1] # this is the number of pixels\n",
    "# The weights are initialized from a normal distribution with standard deviation weight_scale\n",
    "model = TwoLayerNet(input_dim=num_features, hidden_dim=300, num_classes=nb_classes, weight_scale=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32f7f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you can take a look if you want at the initial loss from an untrained network\n",
    "loss, gradients = model.loss(x_train, y_train_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43e3aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple implementation of stochastic gradient descent\n",
    "def sgd(model, gradients, learning_rate):\n",
    "    for p, w in model.params.items():\n",
    "        dw = gradients[p]\n",
    "        new_weights = w - learning_rate * dw\n",
    "        model.params[p] = new_weights\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8316228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one training step\n",
    "def learn(model, x_train, y_train_onehot, learning_rate):\n",
    "    loss, gradients = model.loss(x_train, y_train_onehot)\n",
    "    model = sgd(model, gradients, learning_rate)\n",
    "    return loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81886e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, x, true_values):\n",
    "    scores = model.loss(x)\n",
    "    predictions = numpy.argmax(scores, axis=1)\n",
    "    N = predictions.shape[0]\n",
    "    acc = (true_values == predictions).sum() / N\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49754891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 2.30055, accuracy 0.14\n",
      "epoch 1, loss 2.29904, accuracy 0.18\n",
      "epoch 2, loss 2.29764, accuracy 0.22\n",
      "epoch 3, loss 2.29619, accuracy 0.27\n",
      "epoch 4, loss 2.29435, accuracy 0.33\n",
      "epoch 5, loss 2.29285, accuracy 0.39\n",
      "epoch 6, loss 2.29099, accuracy 0.44\n",
      "epoch 7, loss 2.28940, accuracy 0.48\n",
      "epoch 8, loss 2.28766, accuracy 0.52\n",
      "epoch 9, loss 2.28562, accuracy 0.55\n",
      "epoch 10, loss 2.28408, accuracy 0.57\n",
      "epoch 11, loss 2.28229, accuracy 0.58\n",
      "epoch 12, loss 2.28034, accuracy 0.60\n",
      "epoch 13, loss 2.27820, accuracy 0.61\n",
      "epoch 14, loss 2.27625, accuracy 0.62\n",
      "epoch 15, loss 2.27426, accuracy 0.62\n",
      "epoch 16, loss 2.27117, accuracy 0.63\n",
      "epoch 17, loss 2.26937, accuracy 0.63\n",
      "epoch 18, loss 2.26693, accuracy 0.63\n",
      "epoch 19, loss 2.26485, accuracy 0.63\n",
      "epoch 20, loss 2.26219, accuracy 0.63\n",
      "epoch 21, loss 2.25909, accuracy 0.63\n",
      "epoch 22, loss 2.25647, accuracy 0.63\n",
      "epoch 23, loss 2.25294, accuracy 0.63\n",
      "epoch 24, loss 2.25023, accuracy 0.63\n",
      "epoch 25, loss 2.24735, accuracy 0.62\n",
      "epoch 26, loss 2.24360, accuracy 0.62\n",
      "epoch 27, loss 2.24022, accuracy 0.62\n",
      "epoch 28, loss 2.23643, accuracy 0.62\n",
      "epoch 29, loss 2.23214, accuracy 0.62\n",
      "epoch 30, loss 2.22768, accuracy 0.61\n",
      "epoch 31, loss 2.22428, accuracy 0.61\n",
      "epoch 32, loss 2.22029, accuracy 0.61\n",
      "epoch 33, loss 2.21612, accuracy 0.61\n",
      "epoch 34, loss 2.21190, accuracy 0.61\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m y_train_batch \u001b[38;5;241m=\u001b[39m y_train_onehot[batch_range,:]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# feed the next batch in to do one sgd step\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m loss, model \u001b[38;5;241m=\u001b[39m \u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m losses[i] \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     23\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mlearn\u001b[0;34m(model, x_train, y_train_onehot, learning_rate)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(model, x_train, y_train_onehot, learning_rate):\n\u001b[0;32m----> 3\u001b[0m     loss, gradients \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m sgd(model, gradients, learning_rate)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, model\n",
      "File \u001b[0;32m~/ai-science-training-series/02_neural_networks_python/fc_net.py:70\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mCompute loss and gradient for a minibatch of data.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m  names to gradients of the loss with respect to those parameters.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# forward pass, keeping intermediate values to avoid recomputing\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m out_layer1, cache_layer1 \u001b[38;5;241m=\u001b[39m \u001b[43maffine_relu_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m scores, cache_layer2 \u001b[38;5;241m=\u001b[39m affine_forward(out_layer1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# If y is None then we are in test mode so just return scores\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-science-training-series/02_neural_networks_python/layer_utils.py:18\u001b[0m, in \u001b[0;36maffine_relu_forward\u001b[0;34m(x, w, b)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maffine_relu_forward\u001b[39m(x, w, b):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Convenience layer that perorms an affine transform followed by a ReLU\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    - cache: Object to give to the backward pass\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     a, fc_cache \u001b[38;5;241m=\u001b[39m \u001b[43maffine_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     out, relu_cache \u001b[38;5;241m=\u001b[39m relu_forward(a)\n\u001b[1;32m     20\u001b[0m     cache \u001b[38;5;241m=\u001b[39m (fc_cache, relu_cache)\n",
      "File \u001b[0;32m~/ai-science-training-series/02_neural_networks_python/layers.py:24\u001b[0m, in \u001b[0;36maffine_forward\u001b[0;34m(x, w, b)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maffine_forward\u001b[39m(x, w, b):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Computes the forward pass for an affine (fully-connected) layer.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    - cache: (x, w, b)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m     25\u001b[0m     cache \u001b[38;5;241m=\u001b[39m (x, w, b)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, cache\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here's an example training loop using this two-layer model. Can you do better? \n",
    "learning_rate = 0.01  \n",
    "num_examples = x_train.shape[0]\n",
    "batch_size = 10000\n",
    "num_batches = int(num_examples / batch_size)\n",
    "num_epochs = 100\n",
    "losses = numpy.zeros(num_batches*num_epochs,)\n",
    "indices = numpy.arange(num_examples)\n",
    "\n",
    "i = 0\n",
    "for epoch in range(0, num_epochs):\n",
    "    # in each epoch, we loop over all of the training examples\n",
    "    for step in range(0, num_batches):\n",
    "        # grabbing the next batch\n",
    "        offset = step * batch_size\n",
    "        batch_range = range(offset, offset+batch_size)\n",
    "        x_train_batch = x_train[batch_range, :]\n",
    "        y_train_batch = y_train_onehot[batch_range,:]\n",
    "        \n",
    "        # feed the next batch in to do one sgd step\n",
    "        loss, model = learn(model, x_train_batch, y_train_batch, learning_rate)\n",
    "        losses[i] = loss\n",
    "        i += 1\n",
    "\n",
    "    acc = accuracy(model, x_train, y_train)\n",
    "    print(\"epoch %d, loss %.5f, accuracy %.2f\" % (epoch, loss, acc))\n",
    "    \n",
    "    # reshuffle the data so that we get a new set of batches\n",
    "    numpy.random.shuffle(indices)\n",
    "    x_train = x_train[indices,:]\n",
    "    y_train = y_train[indices] # keep this shuffled the same way for use in accuracy calculation\n",
    "    y_train_onehot = y_train_onehot[indices,:]\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f274c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd5728",
   "metadata": {},
   "source": [
    "# Homework: improve the accuracy of this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf0515",
   "metadata": {},
   "source": [
    "Update this notebook so that the accuracy is improved. How high can you get it? You could change things directly in the notebook, such as increasing the number of epochs, changing the learning weight, changing the width of the hidden layer, etc. If you're more ambitious, you could also try changing the model definition itself by checking out the associated Python files. For example, you could add more layers to the network. The current notebook has a training accuracy of about 43%, but will vary with randomness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda/2022-07-01",
   "language": "python",
   "name": "conda-2022-07-01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
